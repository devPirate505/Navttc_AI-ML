{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ylXMz1dMeL",
        "outputId": "11573d3a-e941-4427-f1e1-6bbfaf130ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g09h-WYldtF_",
        "outputId": "dd3ff321-e3ba-4e4c-afe7-e91a3c7ae5d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Download required NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmIbP0VZdwe8",
        "outputId": "81205a91-e2b2-4d6c-c0ba-fce4a5edd3c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## An example text\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) enables computers to understand human language.\n",
        "It involves steps like text cleaning, tokenization, normalization, stemming, lemmatization,\n",
        "and removing stopwords! NLP applications include chatbots, translators, and sentiment analysis.\"\"\""
      ],
      "metadata": {
        "id": "AntIFxY5d38D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Lowercase and strip\n",
        "clean_text = clean_text.lower().strip()\n",
        "\n",
        "print(\"Cleaned Text:\\n\", clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMCxeq5OeKlf",
        "outputId": "4ea29a26-7762-4dac-87ee-8b1b973011f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " natural language processing nlp enables computers to understand human language \n",
            "it involves steps like text cleaning tokenization normalization stemming lemmatization \n",
            "and removing stopwords nlp applications include chatbots translators and sentiment analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "clean_text = re.sub(r'\\d+', '', clean_text)  # remove numbers\n",
        "print(\"\\nNormalized Text:\\n\", clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKgNPXt8eOYZ",
        "outputId": "8a67d562-de48-4e1e-b306-1931d396fa70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Text:\n",
            " natural language processing nlp enables computers to understand human language \n",
            "it involves steps like text cleaning tokenization normalization stemming lemmatization \n",
            "and removing stopwords nlp applications include chatbots translators and sentiment analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokens:\\n\", sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0syDbmedeUZh",
        "outputId": "2b3b0e2e-260b-4c99-9303-4baf3b609cfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokens:\n",
            " ['Natural Language Processing (NLP) enables computers to understand human language.', 'It involves steps like text cleaning, tokenization, normalization, stemming, lemmatization, \\nand removing stopwords!', 'NLP applications include chatbots, translators, and sentiment analysis.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(clean_text)\n",
        "print(\"\\nWord Tokens:\\n\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNol6uPEeX59",
        "outputId": "b0ea5294-c8d5-4d1c-d36e-2c260e5d3d13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokens:\n",
            " ['natural', 'language', 'processing', 'nlp', 'enables', 'computers', 'to', 'understand', 'human', 'language', 'it', 'involves', 'steps', 'like', 'text', 'cleaning', 'tokenization', 'normalization', 'stemming', 'lemmatization', 'and', 'removing', 'stopwords', 'nlp', 'applications', 'include', 'chatbots', 'translators', 'and', 'sentiment', 'analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "print(\"\\nAfter Stopword Removal:\\n\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a3p3SzBehW2",
        "outputId": "43f16825-ac70-480f-e5b8-aafaa96b06b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stopword Removal:\n",
            " ['natural', 'language', 'processing', 'nlp', 'enables', 'computers', 'understand', 'human', 'language', 'involves', 'steps', 'like', 'text', 'cleaning', 'tokenization', 'normalization', 'stemming', 'lemmatization', 'removing', 'stopwords', 'nlp', 'applications', 'include', 'chatbots', 'translators', 'sentiment', 'analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ7eE-l3e8lL",
        "outputId": "4f13bc18-8b7a-4568-a0c9-3a0eac806b6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(filtered_words)\n",
        "print(\"\\nParts of Speech Tags:\\n\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg8CwVNWekKw",
        "outputId": "5150a398-3977-4ab9-dbfb-bf8336e2a2f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parts of Speech Tags:\n",
            " [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('enables', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('involves', 'VBZ'), ('steps', 'NNS'), ('like', 'IN'), ('text', 'NN'), ('cleaning', 'NN'), ('tokenization', 'NN'), ('normalization', 'NN'), ('stemming', 'VBG'), ('lemmatization', 'NN'), ('removing', 'VBG'), ('stopwords', 'NNS'), ('nlp', 'JJ'), ('applications', 'NNS'), ('include', 'VBP'), ('chatbots', 'NNS'), ('translators', 'NNS'), ('sentiment', 'VBP'), ('analysis', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"\\nNamed Entities, Phrases, and Labels:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}  --->  {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfGSgOJEem0N",
        "outputId": "82469667-bee0-47ab-e1b8-eba497606aa6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entities, Phrases, and Labels:\n",
            "Natural Language Processing  --->  ORG\n",
            "NLP  --->  ORG\n",
            "NLP  --->  ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nAfter Stemming:\\n\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX15P7UKfMbY",
        "outputId": "84dfb86f-edb9-4132-fe39-52c4486f13af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stemming:\n",
            " ['natur', 'languag', 'process', 'nlp', 'enabl', 'comput', 'understand', 'human', 'languag', 'involv', 'step', 'like', 'text', 'clean', 'token', 'normal', 'stem', 'lemmat', 'remov', 'stopword', 'nlp', 'applic', 'includ', 'chatbot', 'translat', 'sentiment', 'analysi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word,pos = 'v') for word in filtered_words]\n",
        "print(\"\\nAfter Lemmatization (NLTK):\\n\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCz7UKwufQk3",
        "outputId": "017a5ba3-3fbc-4c25-f333-33ce06f45ea0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Lemmatization (NLTK):\n",
            " ['natural', 'language', 'process', 'nlp', 'enable', 'computers', 'understand', 'human', 'language', 'involve', 'step', 'like', 'text', 'clean', 'tokenization', 'normalization', 'stem', 'lemmatization', 'remove', 'stopwords', 'nlp', 'applications', 'include', 'chatbots', 'translators', 'sentiment', 'analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join cleaned tokens into one document\n",
        "clean_text = \" \".join(lemmatized_words)\n",
        "corpus = [clean_text]\n",
        "\n",
        "print(\"üßπ Clean Text:\\n\", clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWrhXeVysiY2",
        "outputId": "1e7bc40f-c980-48a1-e638-9dd0f20e60dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Clean Text:\n",
            " natural language process nlp enable computers understand human language involve step like text clean tokenization normalization stem lemmatization remove stopwords nlp applications include chatbots translators sentiment analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ One-Hot Encoding\n",
        "# ====================================================\n",
        "unique_words = list(set(lemmatized_words))\n",
        "one_hot_df = pd.DataFrame(0, index=[0], columns=unique_words)\n",
        "for word in lemmatized_words:\n",
        "    one_hot_df[word] = 1\n",
        "print(\"\\nüîπ One-Hot Encoding:\\n\", one_hot_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1jhAeUTwp7J",
        "outputId": "9d24adcf-c1fb-4278-da80-d5ca3f8df53c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ One-Hot Encoding:\n",
            "    stem  natural  include  chatbots  enable  computers  involve  remove  \\\n",
            "0     1        1        1         1       1          1        1       1   \n",
            "\n",
            "   process  understand  ...  translators  lemmatization  tokenization  step  \\\n",
            "0        1           1  ...            1              1             1     1   \n",
            "\n",
            "   normalization  clean  human  stopwords  applications  nlp  \n",
            "0              1      1      1          1             1    1  \n",
            "\n",
            "[1 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Bag of Words (BoW)\n",
        "# ====================================================\n",
        "bow_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "X_bow = bow_vectorizer.fit_transform(corpus)\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "print(\"\\nüîπ Bag of Words Representation:\\n\", bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G-Y8CHFwzSE",
        "outputId": "4a754b57-3d42-4d81-c5ee-8f9bf64cd936"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Bag of Words Representation:\n",
            "    applications include chatbots  chatbots translators sentiment  \\\n",
            "0                              1                               1   \n",
            "\n",
            "   clean tokenization normalization  computers understand human  \\\n",
            "0                                 1                           1   \n",
            "\n",
            "   enable computers understand  human language involve  \\\n",
            "0                            1                       1   \n",
            "\n",
            "   include chatbots translators  involve step like  language involve step  \\\n",
            "0                             1                  1                      1   \n",
            "\n",
            "   language process nlp  ...  normalization stem lemmatization  \\\n",
            "0                     1  ...                                 1   \n",
            "\n",
            "   process nlp enable  remove stopwords nlp  stem lemmatization remove  \\\n",
            "0                   1                     1                          1   \n",
            "\n",
            "   step like text  stopwords nlp applications  text clean tokenization  \\\n",
            "0               1                           1                        1   \n",
            "\n",
            "   tokenization normalization stem  translators sentiment analysis  \\\n",
            "0                                1                               1   \n",
            "\n",
            "   understand human language  \n",
            "0                          1  \n",
            "\n",
            "[1 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ TF-IDF (Term Frequency‚ÄìInverse Document Frequency)\n",
        "# ====================================================\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nüîπ TF-IDF Representation:\\n\", tfidf_df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvPcqrp4wzfN",
        "outputId": "7f19a550-eb8f-4597-adf1-a045dbe771e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ TF-IDF Representation:\n",
            "    analysis  applications  chatbots  clean  computers  enable  human  include  \\\n",
            "0      0.18          0.18      0.18   0.18       0.18    0.18   0.18     0.18   \n",
            "\n",
            "   involve  language  ...  process  remove  sentiment  stem  step  stopwords  \\\n",
            "0     0.18     0.359  ...     0.18    0.18       0.18  0.18  0.18       0.18   \n",
            "\n",
            "   text  tokenization  translators  understand  \n",
            "0  0.18          0.18         0.18        0.18  \n",
            "\n",
            "[1 rows x 25 columns]\n"
          ]
        }
      ]
    }
  ]
}